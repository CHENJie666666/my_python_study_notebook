{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "# 该模块为自定义模块，封装了构建决策树的基本方法\r\n",
    "from sklearn.datasets import make_classification\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "# 树的棵数\r\n",
    "n_estimators = 10\r\n",
    "# 列抽样最大特征数\r\n",
    "max_features = 15\r\n",
    "# 生成模拟二分类数据集\r\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_redundant=0, n_informative=2,\r\n",
    "                           random_state=1, n_clusters_per_class=1)\r\n",
    "rng = np.random.RandomState(2)\r\n",
    "X += 2 * rng.uniform(size=X.shape)\r\n",
    "# 划分数据集\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\r\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(700, 20) (700,) (300, 20) (300,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# 合并训练数据和标签\r\n",
    "X_y = np.concatenate([X, y.reshape(-1,1)], axis=1)\r\n",
    "np.random.shuffle(X_y)\r\n",
    "m = X_y.shape[0]\r\n",
    "sampling_subsets = []\r\n",
    "\r\n",
    "for _ in range(n_estimators):\r\n",
    "    idx = np.random.choice(m, m, replace=True)\r\n",
    "    bootstrap_Xy = X_y[idx, :]\r\n",
    "    bootstrap_X = bootstrap_Xy[:, :-1]\r\n",
    "    bootstrap_y = bootstrap_Xy[:, -1]\r\n",
    "    sampling_subsets.append([bootstrap_X, bootstrap_y])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "sampling_subsets[0][0].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### 定义二叉特征分裂函数\r\n",
    "def feature_split(X, feature_i, threshold):\r\n",
    "    split_func = None\r\n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\r\n",
    "        split_func = lambda sample: sample[feature_i] >= threshold\r\n",
    "    else:\r\n",
    "        split_func = lambda sample: sample[feature_i] == threshold\r\n",
    "\r\n",
    "    X_left = np.array([sample for sample in X if split_func(sample)])\r\n",
    "    X_right = np.array([sample for sample in X if not split_func(sample)])\r\n",
    "    return np.array([X_left, X_right])\r\n",
    "\r\n",
    "### 计算基尼指数\r\n",
    "def calculate_gini(y):\r\n",
    "    y = y.tolist()\r\n",
    "    probs = [y.count(i)/len(y) for i in np.unique(y)]\r\n",
    "    gini = sum([p*(1-p) for p in probs])\r\n",
    "    return gini"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### 定义树结点\r\n",
    "class TreeNode():\r\n",
    "    def __init__(self, feature_i=None, threshold=None,\r\n",
    "               leaf_value=None, left_branch=None, right_branch=None):\r\n",
    "        # 特征索引\r\n",
    "        self.feature_i = feature_i          \r\n",
    "        # 特征划分阈值\r\n",
    "        self.threshold = threshold \r\n",
    "        # 叶子节点取值\r\n",
    "        self.leaf_value = leaf_value   \r\n",
    "        # 左子树\r\n",
    "        self.left_branch = left_branch     \r\n",
    "        # 右子树\r\n",
    "        self.right_branch = right_branch\r\n",
    "\r\n",
    "\t\t\r\n",
    "### 定义二叉决策树\r\n",
    "class BinaryDecisionTree(object):\r\n",
    "    ### 决策树初始参数\r\n",
    "    def __init__(self, min_samples_split=2, min_gini_impurity=999,\r\n",
    "                 max_depth=float(\"inf\"), loss=None):\r\n",
    "        # 根结点\r\n",
    "        self.root = None  \r\n",
    "        # 节点最小分裂样本数\r\n",
    "        self.min_samples_split = min_samples_split\r\n",
    "        # 节点初始化基尼不纯度\r\n",
    "        self.min_gini_impurity = min_gini_impurity\r\n",
    "        # 树最大深度\r\n",
    "        self.max_depth = max_depth\r\n",
    "        # 基尼不纯度计算函数\r\n",
    "        self.gini_impurity_calculation = None\r\n",
    "        # 叶子节点值预测函数\r\n",
    "        self._leaf_value_calculation = None\r\n",
    "        # 损失函数\r\n",
    "        self.loss = loss\r\n",
    "\r\n",
    "    ### 决策树拟合函数\r\n",
    "    def fit(self, X, y, loss=None):\r\n",
    "        # 递归构建决策树\r\n",
    "        self.root = self._build_tree(X, y)\r\n",
    "        self.loss = None\r\n",
    "\r\n",
    "    ### 决策树构建函数\r\n",
    "    def _build_tree(self, X, y, current_depth=0):\r\n",
    "        # 初始化最小基尼不纯度\r\n",
    "        init_gini_impurity = 999\r\n",
    "        # 初始化最佳特征索引和阈值\r\n",
    "        best_criteria = None    \r\n",
    "        # 初始化数据子集\r\n",
    "        best_sets = None        \r\n",
    "        \r\n",
    "        if len(np.shape(y)) == 1:\r\n",
    "            y = np.expand_dims(y, axis=1)\r\n",
    "\r\n",
    "        # 合并输入和标签\r\n",
    "        Xy = np.concatenate((X, y), axis=1)\r\n",
    "        # 获取样本数和特征数\r\n",
    "        n_samples, n_features = X.shape\r\n",
    "        # 设定决策树构建条件\r\n",
    "        # 训练样本数量大于节点最小分裂样本数且当前树深度小于最大深度\r\n",
    "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\r\n",
    "            # 遍历计算每个特征的基尼不纯度\r\n",
    "            for feature_i in range(n_features):\r\n",
    "                # 获取第i特征的所有取值\r\n",
    "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\r\n",
    "                # 获取第i个特征的唯一取值\r\n",
    "                unique_values = np.unique(feature_values)\r\n",
    "\r\n",
    "                # 遍历取值并寻找最佳特征分裂阈值\r\n",
    "                for threshold in unique_values:\r\n",
    "                    # 特征节点二叉分裂\r\n",
    "                    Xy1, Xy2 = feature_split(Xy, feature_i, threshold)\r\n",
    "                    # 如果分裂后的子集大小都不为0\r\n",
    "                    if len(Xy1) > 0 and len(Xy2) > 0:\r\n",
    "                        # 获取两个子集的标签值\r\n",
    "                        y1 = Xy1[:, n_features:]\r\n",
    "                        y2 = Xy2[:, n_features:]\r\n",
    "\r\n",
    "                        # 计算基尼不纯度\r\n",
    "                        impurity = self.impurity_calculation(y, y1, y2)\r\n",
    "\r\n",
    "                        # 获取最小基尼不纯度\r\n",
    "                        # 最佳特征索引和分裂阈值\r\n",
    "                        if impurity < init_gini_impurity:\r\n",
    "                            init_gini_impurity = impurity\r\n",
    "                            best_criteria = {\"feature_i\": feature_i, \"threshold\": threshold}\r\n",
    "                            best_sets = {\r\n",
    "                                \"leftX\": Xy1[:, :n_features],   \r\n",
    "                                \"lefty\": Xy1[:, n_features:],   \r\n",
    "                                \"rightX\": Xy2[:, :n_features],  \r\n",
    "                                \"righty\": Xy2[:, n_features:]   \r\n",
    "                                }\r\n",
    "        \r\n",
    "        # 如果计算的最小不纯度小于设定的最小不纯度\r\n",
    "        if init_gini_impurity < self.min_gini_impurity:\r\n",
    "            # 分别构建左右子树\r\n",
    "            left_branch = self._build_tree(best_sets[\"leftX\"], best_sets[\"lefty\"], current_depth + 1)\r\n",
    "            right_branch = self._build_tree(best_sets[\"rightX\"], best_sets[\"righty\"], current_depth + 1)\r\n",
    "            return TreeNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\"threshold\"], left_branch=left_branch, right_branch=right_branch)\r\n",
    "\r\n",
    "        # 计算叶子计算取值\r\n",
    "        leaf_value = self._leaf_value_calculation(y)\r\n",
    "        return TreeNode(leaf_value=leaf_value)\r\n",
    "\r\n",
    "    ### 定义二叉树值预测函数\r\n",
    "    def predict_value(self, x, tree=None):\r\n",
    "        if tree is None:\r\n",
    "            tree = self.root\r\n",
    "        # 如果叶子节点已有值，则直接返回已有值\r\n",
    "        if tree.leaf_value is not None:\r\n",
    "            return tree.leaf_value\r\n",
    "        # 选择特征并获取特征值\r\n",
    "        feature_value = x[tree.feature_i]\r\n",
    "        # 判断落入左子树还是右子树\r\n",
    "        branch = tree.right_branch\r\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\r\n",
    "            if feature_value >= tree.threshold:\r\n",
    "                branch = tree.left_branch\r\n",
    "        elif feature_value == tree.threshold:\r\n",
    "            branch = tree.right_branch\r\n",
    "        # 测试子集\r\n",
    "        return self.predict_value(x, branch)\r\n",
    "\r\n",
    "    ### 数据集预测函数\r\n",
    "    def predict(self, X):\r\n",
    "        y_pred = [self.predict_value(sample) for sample in X]\r\n",
    "        return y_pred\r\n",
    "\r\n",
    "\t\t\t\t\r\n",
    "class ClassificationTree(BinaryDecisionTree):\r\n",
    "    ### 定义基尼不纯度计算过程\r\n",
    "    def _calculate_gini_impurity(self, y, y1, y2):\r\n",
    "        p = len(y1) / len(y)\r\n",
    "        gini = calculate_gini(y)\r\n",
    "        gini_impurity = p * calculate_gini(y1) + (1-p) * calculate_gini(y2)\r\n",
    "        return gini_impurity\r\n",
    "    \r\n",
    "    ### 多数投票\r\n",
    "    def _majority_vote(self, y):\r\n",
    "        most_common = None\r\n",
    "        max_count = 0\r\n",
    "        for label in np.unique(y):\r\n",
    "            # 统计多数\r\n",
    "            count = len(y[y == label])\r\n",
    "            if count > max_count:\r\n",
    "                most_common = label\r\n",
    "                max_count = count\r\n",
    "        return most_common\r\n",
    "    \r\n",
    "    # 分类树拟合\r\n",
    "    def fit(self, X, y):\r\n",
    "        self.impurity_calculation = self._calculate_gini_impurity\r\n",
    "        self._leaf_value_calculation = self._majority_vote\r\n",
    "        super(ClassificationTree, self).fit(X, y)\r\n",
    "\r\n",
    "\t\t\r\n",
    "### CART回归树\r\n",
    "class RegressionTree(BinaryDecisionTree):\r\n",
    "\t# 计算方差减少量\r\n",
    "    def _calculate_variance_reduction(self, y, y1, y2):\r\n",
    "        var_tot = np.var(y, axis=0)\r\n",
    "        var_y1 = np.var(y1, axis=0)\r\n",
    "        var_y2 = np.var(y2, axis=0)\r\n",
    "        frac_1 = len(y1) / len(y)\r\n",
    "        frac_2 = len(y2) / len(y)\r\n",
    "        # 计算方差减少量\r\n",
    "        variance_reduction = var_tot - (frac_1 * var_y1 + frac_2 * var_y2)\r\n",
    "        return sum(variance_reduction)\r\n",
    "\r\n",
    "    # 节点值取平均\r\n",
    "    def _mean_of_y(self, y):\r\n",
    "        value = np.mean(y, axis=0)\r\n",
    "        return value if len(value) > 1 else value[0]\r\n",
    "\r\n",
    "\t# 回归树拟合\r\n",
    "    def fit(self, X, y):\r\n",
    "        self.impurity_calculation = self._calculate_variance_reduction\r\n",
    "        self._leaf_value_calculation = self._mean_of_y\r\n",
    "        super(RegressionTree, self).fit(X, y)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# 自助抽样选择训练数据子集\r\n",
    "def bootstrap_sampling(X, y):\r\n",
    "    X_y = np.concatenate([X, y.reshape(-1,1)], axis=1)\r\n",
    "    np.random.shuffle(X_y)\r\n",
    "    n_samples = X.shape[0]\r\n",
    "    sampling_subsets = []\r\n",
    "\r\n",
    "    for _ in range(n_estimators):\r\n",
    "        # 第一个随机性，行抽样\r\n",
    "        idx1 = np.random.choice(n_samples, n_samples, replace=True)\r\n",
    "        bootstrap_Xy = X_y[idx1, :]\r\n",
    "        bootstrap_X = bootstrap_Xy[:, :-1]\r\n",
    "        bootstrap_y = bootstrap_Xy[:, -1]\r\n",
    "        sampling_subsets.append([bootstrap_X, bootstrap_y])\r\n",
    "    return sampling_subsets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "sampling_subsets = bootstrap_sampling(X_train, y_train)\r\n",
    "sub_X, sub_y = sampling_subsets[0]\r\n",
    "print(sub_X.shape, sub_y.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(700, 20) (700,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "trees = []\n",
    "# 基于决策树构建森林\n",
    "for _ in range(n_estimators):\n",
    "    tree = ClassificationTree(min_samples_split=2, min_gini_impurity=999,\n",
    "                              max_depth=3)\n",
    "    trees.append(tree)\n",
    "\n",
    "trees[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<cart.ClassificationTree at 0x1d5e15aaa20>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# 随机森林训练\n",
    "def fit(X, y):\n",
    "    # 对森林中每棵树训练一个双随机抽样子集\n",
    "    n_features = X.shape[1]\n",
    "    sub_sets = bootstrap_sampling(X, y)\n",
    "    for i in range(n_estimators):\n",
    "        sub_X, sub_y = sub_sets[i]\n",
    "        # 第二个随机性，列抽样\n",
    "        idx2 = np.random.choice(n_features, max_features, replace=True)\n",
    "        sub_X = sub_X[:, idx2]\n",
    "        trees[i].fit(sub_X, sub_y)\n",
    "        trees[i].feature_indices = idx2\n",
    "        print('The {}th tree is trained done...'.format(i+1))\n",
    "\n",
    "fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The 1th tree is trained done...\n",
      "The 2th tree is trained done...\n",
      "The 3th tree is trained done...\n",
      "The 4th tree is trained done...\n",
      "The 5th tree is trained done...\n",
      "The 6th tree is trained done...\n",
      "The 7th tree is trained done...\n",
      "The 8th tree is trained done...\n",
      "The 9th tree is trained done...\n",
      "The 10th tree is trained done...\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "y_preds = []\n",
    "for i in range(n_estimators):\n",
    "    idx = trees[i].feature_indices\n",
    "    sub_X = X_test[:, idx]\n",
    "    y_pred = trees[i].predict(sub_X)\n",
    "    y_preds.append(y_pred)\n",
    "    \n",
    "len(y_preds[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "y_preds = np.array(y_preds).T\n",
    "print(y_preds.shape)\n",
    "y_pred = []\n",
    "for y_p in y_preds:\n",
    "    y_pred.append(np.bincount(y_p.astype('int')).argmax())\n",
    "\n",
    "print(y_pred[:10])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(300, 10)\n",
      "[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7366666666666667\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class RandomForest():\r\n",
    "    def __init__(self, n_estimators=100, min_samples_split=2, min_gain=0,\r\n",
    "                 max_depth=float(\"inf\"), max_features=None):\r\n",
    "        # 树的棵树\r\n",
    "        self.n_estimators = n_estimators\r\n",
    "        # 树最小分裂样本数\r\n",
    "        self.min_samples_split = min_samples_split\r\n",
    "        # 最小增益\r\n",
    "        self.min_gain = min_gain\r\n",
    "        # 树最大深度\r\n",
    "        self.max_depth = max_depth\r\n",
    "        # 所使用最大特征数\r\n",
    "        self.max_features = max_features\r\n",
    "\r\n",
    "        self.trees = []\r\n",
    "        # 基于决策树构建森林\r\n",
    "        for _ in range(self.n_estimators):\r\n",
    "            tree = ClassificationTree(min_samples_split=self.min_samples_split, min_impurity=self.min_gain,\r\n",
    "                                      max_depth=self.max_depth)\r\n",
    "            self.trees.append(tree)\r\n",
    "            \r\n",
    "    # 自助抽样\r\n",
    "    def bootstrap_sampling(self, X, y):\r\n",
    "        X_y = np.concatenate([X, y.reshape(-1,1)], axis=1)\r\n",
    "        np.random.shuffle(X_y)\r\n",
    "        n_samples = X.shape[0]\r\n",
    "        sampling_subsets = []\r\n",
    "\r\n",
    "        for _ in range(self.n_estimators):\r\n",
    "            # 第一个随机性，行抽样\r\n",
    "            idx1 = np.random.choice(n_samples, n_samples, replace=True)\r\n",
    "            bootstrap_Xy = X_y[idx1, :]\r\n",
    "            bootstrap_X = bootstrap_Xy[:, :-1]\r\n",
    "            bootstrap_y = bootstrap_Xy[:, -1]\r\n",
    "            sampling_subsets.append([bootstrap_X, bootstrap_y])\r\n",
    "        return sampling_subsets\r\n",
    "            \r\n",
    "    # 随机森林训练\r\n",
    "    def fit(self, X, y):\r\n",
    "        # 对森林中每棵树训练一个双随机抽样子集\r\n",
    "        sub_sets = self.bootstrap_sampling(X, y)\r\n",
    "        n_features = X.shape[1]\r\n",
    "        # 设置max_feature\r\n",
    "        if self.max_features == None:\r\n",
    "            self.max_features = int(np.sqrt(n_features))\r\n",
    "        \r\n",
    "        for i in range(self.n_estimators):\r\n",
    "            # 第二个随机性，列抽样\r\n",
    "            sub_X, sub_y = sub_sets[i]\r\n",
    "            idx2 = np.random.choice(n_features, self.max_features, replace=True)\r\n",
    "            sub_X = sub_X[:, idx2]\r\n",
    "            self.trees[i].fit(sub_X, sub_y)\r\n",
    "            # 保存每次列抽样的列索引，方便预测时每棵树调用\r\n",
    "            self.trees[i].feature_indices = idx2\r\n",
    "            print('The {}th tree is trained done...'.format(i+1))\r\n",
    "    \r\n",
    "    # 随机森林预测\r\n",
    "    def predict(self, X):\r\n",
    "        y_preds = []\r\n",
    "        for i in range(self.n_estimators):\r\n",
    "            idx = self.trees[i].feature_indices\r\n",
    "            sub_X = X[:, idx]\r\n",
    "            y_pred = self.trees[i].predict(sub_X)\r\n",
    "            y_preds.append(y_pred)\r\n",
    "            \r\n",
    "        y_preds = np.array(y_preds).T\r\n",
    "        res = []\r\n",
    "        for j in y_preds:\r\n",
    "            res.append(np.bincount(j.astype('int')).argmax())\r\n",
    "        return res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "rf = RandomForest(n_estimators=10, max_features=15)\r\n",
    "rf.fit(X_train, y_train)\r\n",
    "y_pred = rf.predict(X_test)\r\n",
    "print(accuracy_score(y_test, y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The 1th tree is trained done...\n",
      "The 2th tree is trained done...\n",
      "The 3th tree is trained done...\n",
      "The 4th tree is trained done...\n",
      "The 5th tree is trained done...\n",
      "The 6th tree is trained done...\n",
      "The 7th tree is trained done...\n",
      "The 8th tree is trained done...\n",
      "The 9th tree is trained done...\n",
      "The 10th tree is trained done...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "clf = RandomForestClassifier(max_depth=3, random_state=0)\r\n",
    "clf.fit(X_train, y_train)\r\n",
    "y_pred = clf.predict(X_test)\r\n",
    "print(accuracy_score(y_test, y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.82\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Installation\\anaconda\\install\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}